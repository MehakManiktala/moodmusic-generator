{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "#import preprocessor as p\n",
    "import csv, re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.corpus import words\n",
    "import string\n",
    "import codecs\n",
    "from sklearn.model_selection import train_test_split\n",
    "import itertools\n",
    "from string import digits\n",
    "from autocorrect import spell \n",
    "import wordninja\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "from sklearn.externals import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_abbrv(input_string):\n",
    "    input_string = input_string.split(\" \")\n",
    "    slangs = pd.read_csv(\"../Data/slangs.csv\", delimiter=\",\")\n",
    "    j = 0\n",
    "    for _str in input_string:\n",
    "        _str = re.sub('[^a-zA-Z0-9-_.]', '', _str)\n",
    "        if(_str.upper() in slangs['abbv'].unique()):\n",
    "            idx = slangs.index[slangs['abbv'] == _str]\n",
    "            input_string[j] = slangs['fullform'][idx].str.lower().to_string(index = False)\n",
    "        j = j + 1\n",
    "    return (' '.join(input_string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_contractions(input_string):\n",
    "    input_string = input_string.split(\" \")\n",
    "    contra = pd.read_csv(\"../Data/contractions.csv\", delimiter=\",\")\n",
    "    j = 0\n",
    "    for _str in input_string:\n",
    "        if(_str.lower() in contra['contraction'].unique()):\n",
    "\n",
    "            idx = contra.index[contra['contraction'] == _str]\n",
    "            input_string[j] = contra['fullform'][idx].str.lower().to_string(index = False)\n",
    "        j = j + 1\n",
    "    return (' '.join(input_string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_hashtags(input_string):\n",
    "    input_string = input_string.split(\" \")\n",
    "    j = 0\n",
    "    for _str in input_string:\n",
    "        if('#' in _str):\n",
    "            input_string[j] = _str.replace('#','')\n",
    "        j = j + 1\n",
    "    return (' '.join(input_string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(dataset):\n",
    "\n",
    "    \n",
    "    \n",
    "    #step 1: remove stop words\n",
    "    stop = set(stopwords.words('english'))\n",
    "    not_stop = [\"don't\",\"weren't\",\"doesn't\",\"isn't\",\"aren't\",\"not\",\"needn't\",\"won't\",\"hasn't\",\"mightn't\",\"didn't\",\"haven't\",\"hadn't\",\"shouldn't\",\"wasn't\",\"mustn't\",\"couldn't\"]\n",
    "    stop = (set(stop).difference(not_stop))\n",
    "    dataset = dataset.apply(lambda x:' '.join([word for word in x.split() if word not in (stop)]))\n",
    "\n",
    "    #step 2: no http links, remove @ words\n",
    "    dataset = dataset.apply(lambda x:' '.join([word for word in x.split() if '@' not in word]))\n",
    "\n",
    "    #step 3: remove hastags\n",
    "    dataset = dataset.apply(lambda x: remove_hashtags(x))\n",
    "\n",
    "    #step 4: expand contractions\n",
    "    dataset = dataset.apply(lambda x: expand_contractions(x))\n",
    "\n",
    "    #step 5: expand abbreviations\n",
    "    dataset = dataset.apply(lambda x: expand_abbrv(x))\n",
    "    \n",
    "    #step 6: remove punctuations\n",
    "    punc = string.punctuation\n",
    "    dataset = dataset.apply(lambda x: list(x))\n",
    "    dataset = dataset.apply(lambda x: ''.join([o for o in x if not o in punc]).split())\n",
    "    dataset = dataset.apply(lambda x: ' '.join(x))\n",
    "\n",
    "    #step 7: remove nos.\n",
    "    remove_digits = str.maketrans('', '', digits)\n",
    "    dataset = dataset.str.translate(remove_digits)\n",
    "    \n",
    "    #step 8: shorten elongated words\n",
    "    dataset = dataset.apply(lambda x: ''.join(''.join(s)[:2] for _, s in itertools.groupby(x)))\n",
    "\n",
    "    #step 9: all lower case\n",
    "    dataset = dataset.str.lower()\n",
    "\n",
    "    #step 10: remove punctuations\n",
    "    punc = string.punctuation\n",
    "    dataset = dataset.apply(lambda x: list(x))\n",
    "    dataset = dataset.apply(lambda x: ''.join([o for o in x if not o in punc]).split())\n",
    "    dataset = dataset.apply(lambda x: ' '.join(x))           \n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
