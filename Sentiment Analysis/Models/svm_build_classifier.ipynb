{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn import datasets\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import pandas as pd\n",
    "from nltk.metrics import ConfusionMatrix\n",
    "import numpy as np\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.calibration import CalibratedClassifierCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "emo=5\n",
    "\n",
    "if emo == 2: \n",
    "    dataset_path = '../Data/Datasets/Binary Classification/'\n",
    "else:\n",
    "    dataset_path = '../Data/Datasets/Multiclass Classification/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../Data/Datasets/Multiclass Classification/svm_encode.pkl']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#save the classifier\n",
    "train = pd.read_csv(dataset_path+'train.csv', sep=\",\", header=None,index_col = False)\n",
    "test = pd.read_csv(dataset_path+'test.csv', sep=\",\", header=None,index_col = False)\n",
    "\n",
    "whole_ds = train.append(test,ignore_index=True )\n",
    "whole_ds['tokenized_sents'] = whole_ds.apply(lambda row: nltk.word_tokenize(row[0]), axis=1)\n",
    "\n",
    "onehot_enc = MultiLabelBinarizer()\n",
    "onehot_enc.fit(whole_ds['tokenized_sents'])\n",
    "\n",
    "lsvm = LinearSVC()\n",
    "lsvm = CalibratedClassifierCV(lsvm) \n",
    "lsvm.fit(onehot_enc.transform(whole_ds['tokenized_sents']), whole_ds[1])\n",
    "joblib.dump(lsvm, dataset_path+'svm_classifier.pkl') \n",
    "joblib.dump(onehot_enc,dataset_path+'svm_encode.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8968253968253969\n",
      "0.883289124668435\n",
      "0.8404255319148937\n",
      "0.8829787234042553\n",
      "0.8422459893048129\n",
      "0.8605898123324397\n",
      "0.8847184986595175\n",
      "0.8337801608579088\n",
      "0.8123324396782842\n",
      "0.8766756032171582\n"
     ]
    }
   ],
   "source": [
    "accuracy_svm = []\n",
    "folds = 10\n",
    "\n",
    "for idx in range(1,folds+1):\n",
    "    \n",
    "    train = pd.read_csv(dataset_path+'cv'+str(idx)+'_train.csv', sep=\",\", header=None,index_col = False)\n",
    "    test = pd.read_csv(dataset_path+'cv'+str(idx)+'_test.csv', sep=\",\", header=None,index_col = False)\n",
    "    \n",
    "    #train['tokenized_sents'] = train.apply(lambda _: '', axis=1)\n",
    "    #for index, row in train.iterrows():\n",
    "    #    train['tokenized_sents'][index] = nltk.word_tokenize(row[0])\n",
    "    \n",
    "    #train['tokenized_sents'] = train.apply(lambda row: nltk.word_tokenize(row[0]), axis=1)\n",
    "    #test['tokenized_sents'] = test.apply(lambda row: nltk.word_tokenize(row[0]), axis=1)\n",
    "\n",
    "    train['tokenized_sents'] = [nltk.word_tokenize(row[0]) for index, row in train.iterrows()]\n",
    "    test['tokenized_sents'] = [nltk.word_tokenize(row[0]) for index, row in test.iterrows()]\n",
    "    \n",
    "    lsvm = LinearSVC()\n",
    "    lsvm.fit(onehot_enc.transform(train['tokenized_sents']), train[1])\n",
    "\n",
    "    score = lsvm.score(onehot_enc.transform(test['tokenized_sents']), test[1])\n",
    "    accuracy_svm.append(score)\n",
    "    print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.8968253968253969, 0.883289124668435, 0.8404255319148937, 0.8829787234042553, 0.8422459893048129, 0.8605898123324397, 0.8847184986595175, 0.8337801608579088, 0.8123324396782842, 0.8766756032171582]\n"
     ]
    }
   ],
   "source": [
    "print(accuracy_svm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8613861280863102\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(accuracy_svm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(937, 2)\n",
      "0.8772678762006404\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv(dataset_path+'train.csv', sep=\",\", header=None,index_col = False)\n",
    "test = pd.read_csv(dataset_path+'test.csv', sep=\",\", header=None,index_col = False)\n",
    "\n",
    "print(test.shape)\n",
    "\n",
    "train['tokenized_sents'] = [nltk.word_tokenize(row[0]) for index, row in train.iterrows()]\n",
    "test['tokenized_sents'] = [nltk.word_tokenize(row[0]) for index, row in test.iterrows()]\n",
    "    \n",
    "\n",
    "lsvm = LinearSVC()\n",
    "lsvm.fit(onehot_enc.transform(train['tokenized_sents']), train[1])\n",
    "\n",
    "score = lsvm.score(onehot_enc.transform(test['tokenized_sents']), test[1])\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
