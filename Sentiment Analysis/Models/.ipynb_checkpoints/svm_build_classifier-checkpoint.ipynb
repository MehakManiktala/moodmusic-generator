{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn import datasets\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import pandas as pd\n",
    "from nltk.metrics import ConfusionMatrix\n",
    "import numpy as np\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.calibration import CalibratedClassifierCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "emo=5 #no. of emotions\n",
    "\n",
    "if emo == 2: \n",
    "    dataset_path = '../Data/Datasets/Binary Classification/'\n",
    "else:\n",
    "    dataset_path = '../Data/Datasets/Multiclass Classification/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../Data/Datasets/Multiclass Classification/svm_encode.pkl']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#build the classifier\n",
    "train = pd.read_csv(dataset_path+'train.csv', sep=\",\", header=None,index_col = False)\n",
    "test = pd.read_csv(dataset_path+'test.csv', sep=\",\", header=None,index_col = False)\n",
    "\n",
    "whole_ds = train.append(test,ignore_index=True )\n",
    "whole_ds['tokenized_sents'] = whole_ds.apply(lambda row: nltk.word_tokenize(row[0]), axis=1)\n",
    "\n",
    "onehot_enc = MultiLabelBinarizer()\n",
    "onehot_enc.fit(whole_ds['tokenized_sents'])\n",
    "\n",
    "lsvm = LinearSVC()\n",
    "lsvm = CalibratedClassifierCV(lsvm) \n",
    "lsvm.fit(onehot_enc.transform(whole_ds['tokenized_sents']), whole_ds[1])\n",
    "joblib.dump(lsvm, dataset_path+'svm_classifier.pkl') \n",
    "joblib.dump(onehot_enc,dataset_path+'svm_encode.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Holdout testing accuracy: 0.8772678762006404\n"
     ]
    }
   ],
   "source": [
    "#hold out testing\n",
    "train = pd.read_csv(dataset_path+'train.csv', sep=\",\", header=None,index_col = False)\n",
    "test = pd.read_csv(dataset_path+'test.csv', sep=\",\", header=None,index_col = False)\n",
    "\n",
    "train['tokenized_sents'] = [nltk.word_tokenize(row[0]) for index, row in train.iterrows()]\n",
    "test['tokenized_sents'] = [nltk.word_tokenize(row[0]) for index, row in test.iterrows()]\n",
    "    \n",
    "\n",
    "lsvm = LinearSVC()\n",
    "lsvm.fit(onehot_enc.transform(train['tokenized_sents']), train[1])\n",
    "\n",
    "score = lsvm.score(onehot_enc.transform(test['tokenized_sents']), test[1])\n",
    "print(\"Holdout testing accuracy: {}\".format(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 1: accuracy = 0.8968253968253969\n",
      "Fold: 2: accuracy = 0.883289124668435\n",
      "Fold: 3: accuracy = 0.8404255319148937\n",
      "Fold: 4: accuracy = 0.8829787234042553\n",
      "Fold: 5: accuracy = 0.8422459893048129\n",
      "Fold: 6: accuracy = 0.8605898123324397\n",
      "Fold: 7: accuracy = 0.8847184986595175\n",
      "Fold: 8: accuracy = 0.8337801608579088\n",
      "Fold: 9: accuracy = 0.8123324396782842\n",
      "Fold: 10: accuracy = 0.8766756032171582\n",
      "10 fold Cross Validation accuracy = 0.8613861280863102\n"
     ]
    }
   ],
   "source": [
    "#Cross validation testing\n",
    "accuracy_svm = []\n",
    "folds = 10\n",
    "\n",
    "for idx in range(1,folds+1):\n",
    "    \n",
    "    train = pd.read_csv(dataset_path+'cv'+str(idx)+'_train.csv', sep=\",\", header=None,index_col = False)\n",
    "    test = pd.read_csv(dataset_path+'cv'+str(idx)+'_test.csv', sep=\",\", header=None,index_col = False)\n",
    "    \n",
    "    #train['tokenized_sents'] = train.apply(lambda _: '', axis=1)\n",
    "    #for index, row in train.iterrows():\n",
    "    #    train['tokenized_sents'][index] = nltk.word_tokenize(row[0])\n",
    "    \n",
    "    #train['tokenized_sents'] = train.apply(lambda row: nltk.word_tokenize(row[0]), axis=1)\n",
    "    #test['tokenized_sents'] = test.apply(lambda row: nltk.word_tokenize(row[0]), axis=1)\n",
    "\n",
    "    train['tokenized_sents'] = [nltk.word_tokenize(row[0]) for index, row in train.iterrows()]\n",
    "    test['tokenized_sents'] = [nltk.word_tokenize(row[0]) for index, row in test.iterrows()]\n",
    "    \n",
    "    lsvm = LinearSVC()\n",
    "    lsvm.fit(onehot_enc.transform(train['tokenized_sents']), train[1])\n",
    "\n",
    "    score = lsvm.score(onehot_enc.transform(test['tokenized_sents']), test[1])\n",
    "    accuracy_svm.append(score)\n",
    "    print(\"Fold: {}: accuracy = {}\".format(idx,score))\n",
    "\n",
    "print(\"10 fold Cross Validation accuracy = {}\".format(np.mean(accuracy_svm)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
