{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "#import preprocessor as p\n",
    "import csv, re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.corpus import words\n",
    "import string\n",
    "import codecs\n",
    "from sklearn.model_selection import train_test_split\n",
    "import itertools\n",
    "from string import digits\n",
    "from autocorrect import spell \n",
    "import wordninja\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "from sklearn.externals import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_abbrv(input_string):\n",
    "    input_string = input_string.split(\" \")\n",
    "    slangs = pd.read_csv(\"../Data/slangs.csv\", delimiter=\",\")\n",
    "    j = 0\n",
    "    for _str in input_string:\n",
    "        _str = re.sub('[^a-zA-Z0-9-_.]', '', _str)\n",
    "        if(_str.upper() in slangs['abbv'].unique()):\n",
    "            idx = slangs.index[slangs['abbv'] == _str]\n",
    "            input_string[j] = slangs['fullform'][idx].str.lower().to_string(index = False)\n",
    "        j = j + 1\n",
    "    return (' '.join(input_string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_contractions(input_string):\n",
    "    input_string = input_string.split(\" \")\n",
    "    contra = pd.read_csv(\"../Data/contractions.csv\", delimiter=\",\")\n",
    "    j = 0\n",
    "    for _str in input_string:\n",
    "        if(_str.lower() in contra['contraction'].unique()):\n",
    "\n",
    "            idx = contra.index[contra['contraction'] == _str]\n",
    "            input_string[j] = contra['fullform'][idx].str.lower().to_string(index = False)\n",
    "        j = j + 1\n",
    "    return (' '.join(input_string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_hashtags(input_string):\n",
    "    input_string = input_string.split(\" \")\n",
    "    j = 0\n",
    "    for _str in input_string:\n",
    "        if('#' in _str):\n",
    "            input_string[j] = _str.replace('#','')\n",
    "        j = j + 1\n",
    "    return (' '.join(input_string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Data/rnn_glove_words.pkl']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_dict = dict()\n",
    "glove_words = []\n",
    "#with open('../Dataset/glove.twitter.27B/glove.twitter.27B.25d.txt','rb') as f:\n",
    "with open('../Data/glove.6B/glove.6B.50d.txt','rb') as f:\n",
    "\n",
    "    for idx,ln in enumerate(f):\n",
    "        decoded=False\n",
    "        line=''\n",
    "        for cp in ('cp1252', 'cp850','utf-8','utf8'):\n",
    "            try:\n",
    "                line = ln.decode(cp)\n",
    "                decoded=True\n",
    "                break\n",
    "            except UnicodeDecodeError:\n",
    "                pass\n",
    "        if decoded:\n",
    "            if(idx==0):\n",
    "                continue\n",
    "\n",
    "            split = line.split(' ')\n",
    "\n",
    "            word = split[0]\n",
    "            glove_words.append(word)\n",
    "\n",
    "            features = split[1:]\n",
    "            features = np.array(\n",
    "                [float(val) for val in features]\n",
    "            )\n",
    "            embedding_dict[word] = features\n",
    "\n",
    "number_to_word = glove_words\n",
    "word_to_number = dict((word,idx) for idx,word in enumerate(glove_words))\n",
    "\n",
    "joblib.dump(embedding_dict,'../Data/rnn_embedding_dict.pkl')\n",
    "joblib.dump(glove_words,'../Data/rnn_glove_words.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.2764     2.3994    -1.1009    -1.5421     0.50186   -0.27513\n",
      " -0.84622    0.1746    -0.18847   -0.041907   0.51878    0.61113\n",
      " -0.17486   -0.92846   -0.32452   -0.40843   -0.19901   -1.1288\n",
      " -0.8511     0.55934   -0.92668   -0.18713   -0.94548    0.45426\n",
      " -0.44357   -0.79661   -0.94763   -0.20948   -0.84215   -0.0076193\n",
      "  1.3822     0.6235     0.26689   -0.31898    0.021835  -0.25068\n",
      "  0.65412    0.21796    0.62775    0.97598    0.82678    0.44615\n",
      " -0.022523  -0.92487    0.017586   1.253     -0.83208   -1.4275\n",
      "  0.3922    -0.6229   ]\n"
     ]
    }
   ],
   "source": [
    "print(embedding_dict['saint'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(dataset):\n",
    "\n",
    "    \n",
    "    \n",
    "    #step 1: remove stop words\n",
    "    stop = set(stopwords.words('english'))\n",
    "    not_stop = [\"don't\",\"weren't\",\"doesn't\",\"isn't\",\"aren't\",\"not\",\"needn't\",\"won't\",\"hasn't\",\"mightn't\",\"didn't\",\"haven't\",\"hadn't\",\"shouldn't\",\"wasn't\",\"mustn't\",\"couldn't\"]\n",
    "    stop = (set(stop).difference(not_stop))\n",
    "    dataset = dataset.apply(lambda x:' '.join([word for word in x.split() if word not in (stop)]))\n",
    "\n",
    "    #step 2: no http links, remove @ words\n",
    "    dataset = dataset.apply(lambda x:' '.join([word for word in x.split() if '@' not in word]))\n",
    "\n",
    "    #step 3: remove hastags\n",
    "    dataset = dataset.apply(lambda x: remove_hashtags(x))\n",
    "\n",
    "    #step 4: expand contractions\n",
    "    dataset = dataset.apply(lambda x: expand_contractions(x))\n",
    "\n",
    "    #step 5: expand abbreviations\n",
    "    dataset = dataset.apply(lambda x: expand_abbrv(x))\n",
    "    \n",
    "    #step 6: remove punctuations\n",
    "    punc = string.punctuation\n",
    "    dataset = dataset.apply(lambda x: list(x))\n",
    "    dataset = dataset.apply(lambda x: ''.join([o for o in x if not o in punc]).split())\n",
    "    dataset = dataset.apply(lambda x: ' '.join(x))\n",
    "\n",
    "    #step 7: remove nos.\n",
    "    remove_digits = str.maketrans('', '', digits)\n",
    "    dataset = dataset.str.translate(remove_digits)\n",
    "    \n",
    "    #step 8: shorten elongated words\n",
    "    dataset = dataset.apply(lambda x: ''.join(''.join(s)[:2] for _, s in itertools.groupby(x)))\n",
    "\n",
    "    #step 9: all lower case\n",
    "    dataset = dataset.str.lower()\n",
    "\n",
    "    #step 10: remove punctuations\n",
    "    punc = string.punctuation\n",
    "    dataset = dataset.apply(lambda x: list(x))\n",
    "    dataset = dataset.apply(lambda x: ''.join([o for o in x if not o in punc]).split())\n",
    "    dataset = dataset.apply(lambda x: ' '.join(x))\n",
    "    \n",
    "    #processing 2: separate joined words and spell check\n",
    "\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    english_vocab = set(w.lower() for w in nltk.corpus.words.words())\n",
    "    processed_dataset = []\n",
    "    missing_words = []\n",
    "    number_missing = 0\n",
    "    for el_id,el in enumerate(dataset):\n",
    "        newL = []\n",
    "        word_list = el.split(' ')\n",
    "        for idx,word in enumerate(word_list):\n",
    "            if not word in embedding_dict:\n",
    "                b = spell(word)\n",
    "                if ((lemmatizer.lemmatize(b) in words.words()) and (not word in words.words())):\n",
    "                    newL.append(b)\n",
    "                else:\n",
    "                    a = wordninja.split(word)\n",
    "                    flag = False\n",
    "\n",
    "                    for item in a:\n",
    "                        if not item in words.words():\n",
    "                            flag = True\n",
    "\n",
    "                    if (flag == False and len(a)>1):\n",
    "                       newL.append(\" \".join(a))\n",
    "                    else:\n",
    "                        newL.append(word)\n",
    "                number_missing = number_missing + 1\n",
    "                missing_words.append(word)\n",
    "            else:\n",
    "                newL.append(word)\n",
    "\n",
    "        newE = ' '.join(newL)\n",
    "        processed_dataset.append(newE)\n",
    "        \n",
    "    processed_dataset = np.asarray(processed_dataset) \n",
    "    number_missing_2 = 0\n",
    "    missing_words_2 = []\n",
    "\n",
    "    idx_with_missing_word  = []     \n",
    "    for el_id,el in enumerate(processed_dataset):\n",
    "        changed = False\n",
    "        word_list = el.split(' ')\n",
    "        new_sentence = []\n",
    "        for word in word_list:\n",
    "            if not word in embedding_dict:\n",
    "                changed = True\n",
    "            else:\n",
    "                new_sentence.append(word)\n",
    "\n",
    "        if(changed == True):\n",
    "            processed_dataset[el_id] = \" \".join(new_sentence)\n",
    "\n",
    "    for el_id,el in enumerate(processed_dataset):\n",
    "        word_list = el.split(' ')\n",
    "        for word in word_list:\n",
    "            if not word in embedding_dict:\n",
    "                number_missing_2 = number_missing_2 + 1\n",
    "                missing_words_2.append(word)\n",
    "                idx_with_missing_word.append(el_id)\n",
    "\n",
    "    dataset = dataset.apply(lambda x:' '.join([word for word in x.split() if word not in (stop)]))            \n",
    "                \n",
    "    print(processed_dataset.shape)\n",
    "    return processed_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1616, 4)\n",
      "(821,)\n",
      "(212, 1)\n",
      "(212,)\n",
      "(1701, 4)\n",
      "(834,)\n",
      "(2252, 4)\n",
      "(1115,)\n",
      "(1533, 4)\n",
      "(764,)\n"
     ]
    }
   ],
   "source": [
    "train_files = ['../Data/Raw/joy_train.txt','../Data/Raw/surprise_train.txt','../Data/Raw/anger_train.txt','../Data/Raw/fear_train.txt','../Data/Raw/sadness_train.txt']\n",
    "processed_files = ['../Data/Processed/processed_joy_train.txt','../Data/Processed/processed_surprise_train.txt','../Data/Processed/processed_anger_train.txt','../Data/Processed/processed_fear_train.txt','Data/Processed/processed_sadness_train.txt']\n",
    "for idx, iF in enumerate(train_files):   \n",
    "    dataset = pd.read_csv(iF, sep=\"\\t\",header=None)\n",
    "    print(dataset.shape)\n",
    "    if(idx!=1):\n",
    "        dataset = dataset.loc[dataset[3]>=0.5]\n",
    "        dataset = process_data(dataset[1])\n",
    "    else:\n",
    "        dataset = process_data(dataset[0])\n",
    "    dataset_pd = pd.DataFrame(data=dataset) \n",
    "    dataset_pd.to_csv(processed_files[idx], index=False,header = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
