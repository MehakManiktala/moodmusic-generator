{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.externals import joblib\n",
    "import nltk\n",
    "#import preprocessor as p\n",
    "import csv, re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.corpus import words\n",
    "import string\n",
    "import codecs\n",
    "from sklearn.model_selection import train_test_split\n",
    "import itertools\n",
    "from string import digits\n",
    "from autocorrect import spell \n",
    "import wordninja\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nb_classify(input_sentences, no_of_classes):\n",
    "    \n",
    "    if no_of_classes == 2: \n",
    "        dataset_path = 'Data/Datasets/Binary Classification/'\n",
    "    else:\n",
    "        dataset_path = 'Data/Datasets/Multiclass Classification/'\n",
    "        \n",
    "    clf = joblib.load(dataset_path+'nb_classifier.pkl')\n",
    "    word_features = joblib.load('Data/nb_word_features.pkl')\n",
    "    \n",
    "    pred = np.zeros((len(input_sentences),1))\n",
    "    for idx,line in enumerate(input_sentences):\n",
    "        words = nltk.word_tokenize(line)\n",
    "        featured_item = {i:(i in words) for i in word_features}\n",
    "        pred[idx]  = int(clf.classify(featured_item))\n",
    "        \n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def svm_classify(input_sentences, no_of_classes):\n",
    "    \n",
    "    if no_of_classes == 2: \n",
    "        dataset_path = 'Data/Datasets/Binary Classification/'\n",
    "    else:\n",
    "        dataset_path = 'Data/Datasets/Multiclass Classification/'\n",
    "        \n",
    "    clf = joblib.load(dataset_path+'svm_classifier.pkl')\n",
    "    onehot_enc = joblib.load(dataset_path+'svm_encode.pkl')\n",
    "    vocabulary = joblib.load('Data/svm_vocabulary.pkl')\n",
    "    \n",
    "    tokenized_input_sent = [word_tokenize(s) for s in input_sentences]\n",
    "    \n",
    "    for line in tokenized_input_sent:\n",
    "        for w in reversed(line):\n",
    "            if(w not in vocabulary):\n",
    "                line.remove(w)\n",
    "    pred = clf.predict(onehot_enc.transform(tokenized_input_sent))\n",
    "    \n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ens_classify(input_sentences, no_of_classes):\n",
    "    \n",
    "    if no_of_classes == 2: \n",
    "        dataset_path = 'Data/Datasets/Binary Classification/'\n",
    "    else:\n",
    "        dataset_path = 'Data/Datasets/Multiclass Classification/'\n",
    "        \n",
    "    clf = joblib.load(dataset_path+'svm_classifier.pkl')\n",
    "    onehot_enc = joblib.load(dataset_path+'svm_encode.pkl')\n",
    "    vocabulary = joblib.load('Data/svm_vocabulary.pkl')\n",
    "    \n",
    "    tokenized_input_sent = [word_tokenize(s) for s in input_sentences]\n",
    "    \n",
    "    for line in tokenized_input_sent:\n",
    "        for w in reversed(line):\n",
    "            if(w not in vocabulary):\n",
    "                line.remove(w)\n",
    "    svm_pred_prob = clf.predict_proba(onehot_enc.transform(tokenized_input_sent))\n",
    "    \n",
    "    clf = joblib.load(dataset_path+'nb_classifier.pkl')\n",
    "    word_features = joblib.load('Data/nb_word_features.pkl')\n",
    "    \n",
    "    nb_pred_prob = np.zeros((len(input_sentences),5))\n",
    "    for idx,line in enumerate(input_sentences):\n",
    "        words = nltk.word_tokenize(line)\n",
    "        featured_item = {i:(i in words) for i in word_features}\n",
    "        p =clf.prob_classify(featured_item)\n",
    "        for label in p.samples():\n",
    "            nb_pred_prob[idx][label] = p.prob(label)\n",
    "    \n",
    "    m=np.zeros((len(input_sentences),5))\n",
    "    for idx,item in enumerate(nb_pred_prob):\n",
    "        m[idx] = np.average([svm_pred_prob[idx], item], axis=0)\n",
    "    ensemble_pred = m.argmax(axis=1)\n",
    "    \n",
    "    return ensemble_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_abbrv(input_string):\n",
    "    input_string = input_string.split(\" \")\n",
    "    slangs = pd.read_csv(\"Data/slangs.csv\", delimiter=\",\")\n",
    "    j = 0\n",
    "    for _str in input_string:\n",
    "        _str = re.sub('[^a-zA-Z0-9-_.]', '', _str)\n",
    "        if(_str.upper() in slangs['abbv'].unique()):\n",
    "            idx = slangs.index[slangs['abbv'] == _str]\n",
    "            input_string[j] = slangs['fullform'][idx].str.lower().to_string(index = False)\n",
    "        j = j + 1\n",
    "    return (' '.join(input_string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_contractions(input_string):\n",
    "    input_string = input_string.split(\" \")\n",
    "    contra = pd.read_csv(\"Data/contractions.csv\", delimiter=\",\")\n",
    "    j = 0\n",
    "    for _str in input_string:\n",
    "        if(_str.lower() in contra['contraction'].unique()):\n",
    "\n",
    "            idx = contra.index[contra['contraction'] == _str]\n",
    "            input_string[j] = contra['fullform'][idx].str.lower().to_string(index = False)\n",
    "        j = j + 1\n",
    "    return (' '.join(input_string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_hashtags(input_string):\n",
    "    input_string = input_string.split(\" \")\n",
    "    j = 0\n",
    "    for _str in input_string:\n",
    "        if('#' in _str):\n",
    "            input_string[j] = _str.replace('#','')\n",
    "        j = j + 1\n",
    "    return (' '.join(input_string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(dataset):\n",
    "\n",
    "    \n",
    "    \n",
    "    #step 1: remove stop words\n",
    "    stop = set(stopwords.words('english'))\n",
    "    not_stop = [\"don't\",\"weren't\",\"doesn't\",\"isn't\",\"aren't\",\"not\",\"needn't\",\"won't\",\"hasn't\",\"mightn't\",\"didn't\",\"haven't\",\"hadn't\",\"shouldn't\",\"wasn't\",\"mustn't\",\"couldn't\"]\n",
    "    stop = (set(stop).difference(not_stop))\n",
    "    dataset = dataset.apply(lambda x:' '.join([word for word in x.split() if word not in (stop)]))\n",
    "\n",
    "    #step 2: no http links, remove @ words\n",
    "    dataset = dataset.apply(lambda x:' '.join([word for word in x.split() if '@' not in word]))\n",
    "\n",
    "    #step 3: remove hastags\n",
    "    dataset = dataset.apply(lambda x: remove_hashtags(x))\n",
    "\n",
    "    #step 4: expand contractions\n",
    "    dataset = dataset.apply(lambda x: expand_contractions(x))\n",
    "\n",
    "    #step 5: expand abbreviations\n",
    "    dataset = dataset.apply(lambda x: expand_abbrv(x))\n",
    "    \n",
    "    #step 6: remove punctuations\n",
    "    punc = string.punctuation\n",
    "    dataset = dataset.apply(lambda x: list(x))\n",
    "    dataset = dataset.apply(lambda x: ''.join([o for o in x if not o in punc]).split())\n",
    "    dataset = dataset.apply(lambda x: ' '.join(x))\n",
    "\n",
    "    #step 7: remove nos.\n",
    "    remove_digits = str.maketrans('', '', digits)\n",
    "    dataset = dataset.str.translate(remove_digits)\n",
    "    \n",
    "    #step 8: shorten elongated words\n",
    "    dataset = dataset.apply(lambda x: ''.join(''.join(s)[:2] for _, s in itertools.groupby(x)))\n",
    "\n",
    "    #step 9: all lower case\n",
    "    dataset = dataset.str.lower()\n",
    "\n",
    "    #step 10: remove punctuations\n",
    "    punc = string.punctuation\n",
    "    dataset = dataset.apply(lambda x: list(x))\n",
    "    dataset = dataset.apply(lambda x: ''.join([o for o in x if not o in punc]).split())\n",
    "    dataset = dataset.apply(lambda x: ' '.join(x))           \n",
    "    \n",
    "    return dataset"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
